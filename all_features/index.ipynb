{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 19:57:09.261149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748732229.368457 3182053 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748732229.399649 3182053 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748732229.632488 3182053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748732229.632521 3182053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748732229.632523 3182053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748732229.632525 3182053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 19:57:09.658447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset_fz_cz.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_columns = df.iloc[:, :1500]\n",
    "def safe_complex(x):\n",
    "    try:\n",
    "        return complex(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "fft_complex = fft_columns.map(safe_complex)\n",
    "df.iloc[:, :1500] = fft_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['classe']\n",
    "X = df.drop(['classe','voluntario'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão em conjunto de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rec_f0 = X_train.iloc[:, :500]\n",
    "X_test_rec_f0 = X_test.iloc[:, :500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:,:1500] = X_train.iloc[:,:1500].map(lambda x: np.abs(x))\n",
    "X_test.iloc[:,:1500] = X_test.iloc[:,:1500].map(lambda x: np.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_f0 = list(range(500)) + [1500] + list(range(1503, X_train.shape[1]))\n",
    "X_train_f0 = X_train.iloc[:, indice_f0]\n",
    "X_test_f0 = X_test.iloc[:, indice_f0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_h2 = list(range(1000)) + [1500, 1501] + list(range(1503, X_train.shape[1]))\n",
    "X_train_h2 = X_train.iloc[:, indice_h2]\n",
    "X_test_h2 = X_test.iloc[:, indice_h2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar Variáveis Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_columns = ['Faixa_Moduladora', \n",
    "                     'Intensidade_db', \n",
    "                     'Canal']\n",
    "\n",
    "numerical_columns = ['Freq_analisada_Hz']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_f0 = preprocessor.fit_transform(X_train_f0)\n",
    "X_test_f0 = preprocessor.transform(X_test_f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_columns = ['Faixa_Moduladora', \n",
    "                     'Intensidade_db', \n",
    "                     'Canal']\n",
    "\n",
    "numerical_columns = ['Freq_analisada_Hz', 'Freq_h2_analisada_Hz',]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_h2 = preprocessor.fit_transform(X_train_h2)\n",
    "X_test_h2 = preprocessor.transform(X_test_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "categorical_columns = ['Faixa_Moduladora', \n",
    "                     'Intensidade_db', \n",
    "                     'Canal']\n",
    "\n",
    "numerical_columns = ['Freq_analisada_Hz', \n",
    "                     'Freq_h2_analisada_Hz',\n",
    "                     'Freq_h3_analisada_Hz']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o ColumnTransformer, as colunas após o pré-processamento podem ter nomes alterados (ex.: Faixa_40, Faixa_80). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_processadas = preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_parquet('f0_h2_h3/X_train.parquet', index=False)\n",
    "pd.DataFrame(X_test).to_parquet('f0_h2_h3/X_test.parquet', index=False)\n",
    "pd.DataFrame(y_train).to_parquet('f0_h2_h3/y_train.parquet', index=False)\n",
    "pd.DataFrame(y_test).to_parquet('f0_h2_h3/y_test.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rec_f0.to_csv('f0/X_train_rec_f0.csv', index=False)\n",
    "X_test_rec_f0.to_csv('f0/X_test_rec_f0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_f0).to_parquet('f0/X_train_f0.parquet', index=False)\n",
    "pd.DataFrame(X_test_f0).to_parquet('f0/X_test_f0.parquet', index=False)\n",
    "pd.DataFrame(y_train).to_parquet('f0/y_train_f0.parquet', index=False)\n",
    "pd.DataFrame(y_test).to_parquet('f0/y_test_f0.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_h2).to_parquet('f0_h2/X_train_h2.parquet', index=False)\n",
    "pd.DataFrame(X_test_h2).to_parquet('f0_h2/X_test_h2.parquet', index=False)\n",
    "pd.DataFrame(y_train).to_parquet('f0_h2/y_train_h2.parquet', index=False)\n",
    "pd.DataFrame(y_test).to_parquet('f0_h2/y_test_h2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até aqui, foi realizado a separação do X e Y, nos quais foram separados em conjunto de teste e treinamento. Além disso, os conjuntos de treino para os modelos de ML foram transformados para pegar apenas os módulos, já os em formato retangular, será utilizado na CSM e MSC. Obtendo assim, 4 datasets finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desenvolvimento dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f0 = pd.read_parquet('f0/X_train_f0.parquet')\n",
    "X_test_f0 = pd.read_parquet('f0/X_test_f0.parquet')\n",
    "y_train_f0 = pd.read_parquet('f0/y_train_f0.parquet')\n",
    "y_test_f0 = pd.read_parquet('f0/y_test_f0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_h2 = pd.read_parquet('f0_h2/X_train_h2.parquet')\n",
    "X_test_h2 = pd.read_parquet('f0_h2/X_test_h2.parquet')\n",
    "y_train_h2 = pd.read_parquet('f0_h2/y_train_h2.parquet')\n",
    "y_test_h2 = pd.read_parquet('f0_h2/y_test_h2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('f0_h2_h3/X_train.parquet')\n",
    "X_test = pd.read_parquet('f0_h2_h3/X_test.parquet')\n",
    "y_train = pd.read_parquet('f0_h2_h3/y_train.parquet')\n",
    "y_test = pd.read_parquet('f0_h2_h3/y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'Fundamental': {\n",
    "        'X_train': X_train_f0,\n",
    "        'y_train': y_train_f0,\n",
    "        'X_test': X_test_f0,\n",
    "        'y_test': y_test_f0\n",
    "    },\n",
    "    'Fundamental + 2º Harmônico': {\n",
    "        'X_train': X_train_h2,\n",
    "        'y_train': y_train_h2,\n",
    "        'X_test': X_test_h2,\n",
    "        'y_test': y_test_h2\n",
    "    },\n",
    "    'Fundamental + 2º + 3º Harmônicos': {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RNA Padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial, input_shape):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 100)\n",
    "    units = trial.suggest_int('units', 32, 512, step=32)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA variando parametros por camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_layer(trial, input_shape):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 50)\n",
    "    units = []\n",
    "    dropouts = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        units.append(trial.suggest_int(f'units_{i}', 32, 512, step=32))\n",
    "        dropouts.append(trial.suggest_float(f'dropout_{i}', 0.0, 0.5))\n",
    "\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
    "    l2_weight = trial.suggest_float('l2_weight', 1e-5, 1e-2, log=True)\n",
    "    l1_weight = trial.suggest_float('l1_weight', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(shape=input_shape))\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        # activation_function = trial.suggest_categorical(f'activation_{i}', ['relu', 'leaky_relu', 'elu'])\n",
    "        # if activation_function == 'leaky_relu':\n",
    "        #     model.add(layers.Dense(units[i], kernel_regularizer=l1_l2(l1=l1_weight, l2=l2_weight)))\n",
    "        #     model.add(layers.LeakyReLU())\n",
    "        # else:\n",
    "        #     model.add(layers.Dense(units[i], activation=activation_function, kernel_regularizer=l1_l2(l1=l1_weight, l2=l2_weight)))\n",
    "            \n",
    "        model.add(layers.Dense(units[i], activation='relu', kernel_regularizer=l1_l2(l1=l1_weight, l2=l2_weight)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropouts[i]))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=tf.keras.optimizers.get({\n",
    "        #     'class_name': optimizer_name,\n",
    "        #     'config': {\n",
    "        #         'learning_rate': learning_rate\n",
    "        #     }\n",
    "        # }),\n",
    "        tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RNA + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, labels, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(labels[i + seq_length - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train_f0_seq, y_train_f0_seq = create_sequence(X_train_f0.values, y_train_f0.values, 20)\n",
    "X_test_f0_seq, y_test_f0_seq = create_sequence(X_test_f0.values, y_test_f0.values, 20)\n",
    "\n",
    "X_train_h2_seq, y_train_h2_seq = create_sequence(X_train_h2.values, y_train_h2.values, 20)\n",
    "X_test_h2_seq, y_test_h2_seq = create_sequence(X_test_h2.values, y_test_h2.values, 20)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequence(X_train.values, y_train.values, 20)\n",
    "X_test_seq, y_test_seq = create_sequence(X_test.values, y_test.values, 20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_seq = {\n",
    "    'Fundamental': {\n",
    "        'X_train': X_train_f0_seq,\n",
    "        'y_train': y_train_f0_seq,\n",
    "        'X_test': X_test_f0_seq,\n",
    "        'y_test': y_test_f0_seq\n",
    "    },\n",
    "    'Fundamental + 2º Harmônico': {\n",
    "        'X_train': X_train_h2_seq,\n",
    "        'y_train': y_train_h2_seq,\n",
    "        'X_test': X_test_h2_seq,\n",
    "        'y_test': y_test_h2_seq\n",
    "    },\n",
    "    'Fundamental + 2º + 3º Harmônicos': {\n",
    "        'X_train': X_train_seq,\n",
    "        'y_train': y_train_seq,\n",
    "        'X_test': X_test_seq,\n",
    "        'y_test': y_test_seq\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(trial, input_shape):\n",
    "    # Hiperparâmetros para LSTM\n",
    "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 10)\n",
    "    lstm_units = [trial.suggest_int(f'lstm_units_{i}', 32, 256) for i in range(n_lstm_layers)]\n",
    "    lstm_dropout = [trial.suggest_float(f'lstm_dropout_{i}', 0.0, 0.5) for i in range(n_lstm_layers)]\n",
    "    \n",
    "    # Hiperparâmetros para Dense\n",
    "    n_dense_layers = trial.suggest_int('n_dense_layers', 1, 50)\n",
    "    dense_units = [trial.suggest_int(f'dense_units_{i}', 32, 512) for i in range(n_dense_layers)]\n",
    "    dense_dropout = [trial.suggest_float(f'dense_dropout_{i}', 0.0, 0.5) for i in range(n_dense_layers)]\n",
    "    \n",
    "    # Construção do modelo\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))  # (look_back, n_features)\n",
    "    \n",
    "    # Camadas LSTM\n",
    "    for i in range(n_lstm_layers):\n",
    "        return_sequences = (i < n_lstm_layers - 1)  # Retorna sequências se não for a última camada\n",
    "        model.add(layers.LSTM(\n",
    "            lstm_units[i],\n",
    "            return_sequences=return_sequences,\n",
    "            kernel_regularizer=l2(trial.suggest_float('l2_weight', 1e-5, 1e-2))\n",
    "        ))\n",
    "        model.add(layers.Dropout(lstm_dropout[i]))\n",
    "    \n",
    "    # Camadas Densas\n",
    "    for i in range(n_dense_layers):\n",
    "        model.add(layers.Dense(dense_units[i], activation='relu'))\n",
    "        model.add(layers.Dropout(dense_dropout[i]))\n",
    "    \n",
    "    # Saída\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(data, labels, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(labels[i + seq_length - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train_f0_seq, y_train_f0_seq = create_sequence(X_train_f0.values, y_train_f0.values, 20)\n",
    "X_test_f0_seq, y_test_f0_seq = create_sequence(X_test_f0.values, y_test_f0.values, 20)\n",
    "\n",
    "X_train_h2_seq, y_train_h2_seq = create_sequence(X_train_h2.values, y_train_h2.values, 20)\n",
    "X_test_h2_seq, y_test_h2_seq = create_sequence(X_test_h2.values, y_test_h2.values, 20)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequence(X_train.values, y_train.values, 20)\n",
    "X_test_seq, y_test_seq = create_sequence(X_test.values, y_test.values, 20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_seq = {\n",
    "    'Fundamental': {\n",
    "        'X_train': X_train_f0_seq,\n",
    "        'y_train': y_train_f0_seq,\n",
    "        'X_test': X_test_f0_seq,\n",
    "        'y_test': y_test_f0_seq\n",
    "    },\n",
    "    'Fundamental + 2º Harmônico': {\n",
    "        'X_train': X_train_h2_seq,\n",
    "        'y_train': y_train_h2_seq,\n",
    "        'X_test': X_test_h2_seq,\n",
    "        'y_test': y_test_h2_seq\n",
    "    },\n",
    "    'Fundamental + 2º + 3º Harmônicos': {\n",
    "        'X_train': X_train_seq,\n",
    "        'y_train': y_train_seq,\n",
    "        'X_test': X_test_seq,\n",
    "        'y_test': y_test_seq\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(trial, input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    current_seq_length = input_shape[0] # Comprimento da sequência inicial\n",
    "\n",
    "    n_cnn_layers = trial.suggest_int('n_cnn_layers', 1, 3) # Número de blocos CNN (Conv1D + MaxPool + Dropout)\n",
    "\n",
    "    # Hiperparâmetros para as camadas CNN\n",
    "    for i in range(n_cnn_layers):\n",
    "        filters = trial.suggest_int(f'cnn_filters_{i}', 32, 256, step=32)\n",
    "        \n",
    "        # Sugere kernel_size DENTRO do loop, limitado pelo current_seq_length\n",
    "        max_kernel_size = min(current_seq_length, 5) # Maximo 5, ou o que restar da sequência\n",
    "        if max_kernel_size < 2: # Se for menor que 2, não podemos usar kernel_size 2 ou maior\n",
    "            raise optuna.exceptions.TrialPruned(\n",
    "                f\"Comprimento da sequência ({current_seq_length}) é muito pequeno para kernel_size >= 2 na camada CNN {i+1}. Trial podado.\"\n",
    "            )\n",
    "        kernel_size = trial.suggest_int(f'cnn_kernel_size_{i}', 2, max_kernel_size)\n",
    "\n",
    "        # Calcula o novo comprimento da sequência após a Conv1D (padding='valid')\n",
    "        new_seq_length_after_conv = (current_seq_length - kernel_size) + 1\n",
    "        \n",
    "        # Sugere pool_size DENTRO do loop, limitado pelo new_seq_length_after_conv\n",
    "        max_pooling_size = min(new_seq_length_after_conv, 4) # Maximo 4, ou o que restar após Conv1D\n",
    "        if max_pooling_size < 2: # Se for menor que 2, não podemos usar pool_size 2 ou maior\n",
    "            raise optuna.exceptions.TrialPruned(\n",
    "                f\"Comprimento da sequência após Conv1D ({new_seq_length_after_conv}) é muito pequeno para pool_size >= 2 na camada CNN {i+1}. Trial podado.\"\n",
    "            )\n",
    "        pool_size = trial.suggest_int(f'cnn_pooling_size_{i}', 2, max_pooling_size)\n",
    "        \n",
    "        cnn_dropout_rate = trial.suggest_float(f'cnn_dropout_{i}', 0.0, 0.5)\n",
    "\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='valid' # Redução de dimensão\n",
    "        ))\n",
    "        model.add(layers.MaxPooling1D(pool_size=pool_size))\n",
    "        model.add(layers.Dropout(cnn_dropout_rate))\n",
    "\n",
    "        # Atualiza o comprimento da sequência para a próxima iteração do loop CNN\n",
    "        current_seq_length = int(np.floor(new_seq_length_after_conv / pool_size))\n",
    "        \n",
    "        # Se o comprimento da sequência se tornar 0 ou negativo antes da última camada CNN, podar o trial\n",
    "        if current_seq_length <= 0 and i < n_cnn_layers - 1:\n",
    "            raise optuna.exceptions.TrialPruned(\n",
    "                f\"Comprimento da sequência se tornou <= 0 após a camada CNN {i+1}. Trial podado.\"\n",
    "            )\n",
    "\n",
    "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 5)\n",
    "    lstm_units = [trial.suggest_int(f'lstm_units_{i}', 32, 256) for i in range(n_lstm_layers)]\n",
    "    lstm_dropout = [trial.suggest_float(f'lstm_dropout_{i}', 0.0, 0.5) for i in range(n_lstm_layers)]\n",
    "    \n",
    "    n_dense_layers = trial.suggest_int('n_dense_layers', 1, 5)\n",
    "    dense_units = [trial.suggest_int(f'dense_units_{i}', 32, 512) for i in range(n_dense_layers)]\n",
    "    dense_dropout = [trial.suggest_float(f'dense_dropout_{i}', 0.0, 0.5) for i in range(n_dense_layers)]\n",
    "    # Camadas LSTM\n",
    "    for i in range(n_lstm_layers):\n",
    "        return_sequences = (i < n_lstm_layers - 1)\n",
    "        model.add(layers.LSTM(\n",
    "            lstm_units[i],\n",
    "            return_sequences=return_sequences,\n",
    "            kernel_regularizer=l2(trial.suggest_float('l2_weight', 1e-5, 1e-2, log=True))\n",
    "        ))\n",
    "        model.add(layers.Dropout(lstm_dropout[i]))\n",
    "\n",
    "    # Camadas Densas\n",
    "    for i in range(n_dense_layers):\n",
    "        model.add(layers.Dense(dense_units[i], activation='relu'))\n",
    "        model.add(layers.Dropout(dense_dropout[i]))\n",
    "\n",
    "    # Saída\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Objetiva para todas as frequências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train):\n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    n_split = 5\n",
    "    kf = StratifiedKFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "    fold_aucs = []\n",
    "\n",
    "    y_train_1d = y_train.squeeze().astype(int)\n",
    "    classes = np.unique(y_train_1d)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train_1d)\n",
    "    class_weight = {i: w for i, w in zip(classes, class_weights)}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train_1d)):\n",
    "        print(f\"=== Iniciando Fold {fold + 1} de {n_split} ===\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_1d[train_idx], y_train_1d[val_idx]\n",
    "\n",
    "        model = create_model_layer(trial, input_shape)  # Mudar se quer RNA\n",
    "        #model = create_lstm_model(trial, input_shape) #Mudar se quer RNA ou RNA + LSTM ou CNN + LSTM\n",
    "        #model = create_cnn_lstm_model(trial, input_shape)  # CNN + LSTM\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=30,\n",
    "            verbose=0,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=20,\n",
    "            mode='max',\n",
    "            min_lr=1e-7,\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val_fold, y_val_fold),\n",
    "            callbacks=[callback, lr_scheduler],\n",
    "            verbose=1,\n",
    "            class_weight=class_weight\n",
    "        )\n",
    "\n",
    "        best_auc_fold = max(history.history['val_auc'])\n",
    "        fold_aucs.append(best_auc_fold)\n",
    "        print(f\"AUC do Fold {fold + 1}: {best_auc_fold:.4f}\")\n",
    "\n",
    "        del model\n",
    "        del callback, lr_scheduler, history\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "    print(\"=== Fim dos Folds ===\")\n",
    "    \n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    print(f\"AUC Médio dos Folds: {mean_auc:.4f}\")\n",
    "\n",
    "    return mean_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Modelo com os melhores hiperparametros - RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_final_model(best_params, input_shape):\n",
    "    n_layers = best_params['n_layers']\n",
    "    units = best_params['units']\n",
    "    dropout = best_params['dropout']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "   \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model_layer(best_params, input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(shape=input_shape))\n",
    "    \n",
    "    # Extrai hiperparâmetros por camada\n",
    "    n_layers = best_params['n_layers']\n",
    "    units = [best_params[f'units_{i}'] for i in range(n_layers)]\n",
    "    dropouts = [best_params[f'dropout_{i}'] for i in range(n_layers)]\n",
    "    \n",
    "    # Constrói as camadas\n",
    "    for i in range(n_layers):\n",
    "        # activation_function = best_params.get(f'activation_{i}', 'relu')\n",
    "        # if activation_function == 'leaky_relu':\n",
    "        #     model.add(layers.Dense(units[i], kernel_regularizer=l1_l2(l1=best_params['l1_weight'], l2=best_params['l2_weight'])))\n",
    "        #     model.add(layers.LeakyReLU(alpha=0.1))\n",
    "        # else:\n",
    "        #     model.add(layers.Dense(units[i], activation=activation_function, kernel_regularizer=l1_l2(l1=best_params['l1_weight'], l2=best_params['l2_weight'])))\n",
    "        model.add(layers.Dense(units[i], activation='relu', kernel_regularizer=l1_l2(l1=best_params['l1_weight'], l2=best_params['l2_weight'])))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dropouts[i]))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        #optimizer=tf.keras.optimizers.get(best_params['optimizer'])(learning_rate=best_params['learning_rate']),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_final_model_lstm(best_params, input_shape):\n",
    "    \n",
    "    n_lstm_layers = best_params['n_lstm_layers']\n",
    "    lstm_dropout = [best_params[f'lstm_dropout_{i}'] for i in range(n_lstm_layers)]\n",
    "    lstm_units = [best_params[f'lstm_units_{i}'] for i in range(n_lstm_layers)]\n",
    "    \n",
    "    n_dense_layers = best_params['n_dense_layers']\n",
    "    dense_units = [best_params[f'dense_units_{i}'] for i in range(n_dense_layers)]\n",
    "    dense_dropout = [best_params[f'dense_dropout_{i}'] for i in range(n_dense_layers)]\n",
    "    \n",
    "    # Construção do modelo\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))  # (look_back, n_features)\n",
    "    \n",
    "    # Camadas LSTM\n",
    "    for i in range(n_lstm_layers):\n",
    "        return_sequences = (i < n_lstm_layers - 1)  # Retorna sequências se não for a última camada\n",
    "        model.add(layers.LSTM(\n",
    "            lstm_units[i],\n",
    "            return_sequences=return_sequences,\n",
    "            kernel_regularizer=l2(best_params['l2_weight'])\n",
    "        ))\n",
    "        model.add(layers.Dropout(lstm_dropout[i]))\n",
    "    \n",
    "    # Camadas Densas\n",
    "    for i in range(n_dense_layers):\n",
    "        model.add(layers.Dense(dense_units[i], activation='relu'))\n",
    "        model.add(layers.Dropout(dense_dropout[i]))\n",
    "    \n",
    "    # Saída\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_final_model_cnn_lstm(best_params, input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    current_seq_length = input_shape[0]\n",
    "    \n",
    "    n_cnn_layers = best_params['n_cnn_layers']\n",
    "\n",
    "    # Camadas Conv1D - Construídas com os melhores parâmetros por camada\n",
    "    for i in range(n_cnn_layers):\n",
    "        filters = best_params[f'cnn_filters_{i}']\n",
    "        # Obtém os kernel_size e pool_size específicos desta camada do best_params\n",
    "        kernel_size = best_params[f'cnn_kernel_size_{i}']\n",
    "        pool_size = best_params[f'cnn_pooling_size_{i}']\n",
    "        cnn_dropout_rate = best_params[f'cnn_dropout_{i}']\n",
    "\n",
    "        # Recalcula ajustados para o modelo final, garantindo validade\n",
    "        new_seq_length_after_conv = (current_seq_length - kernel_size) + 1\n",
    "        adjusted_pool_size = min(pool_size, max(1, new_seq_length_after_conv))\n",
    "\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            padding='valid'\n",
    "        ))\n",
    "        model.add(layers.MaxPooling1D(pool_size=adjusted_pool_size))\n",
    "        model.add(layers.Dropout(cnn_dropout_rate))\n",
    "\n",
    "        current_seq_length = int(np.floor(new_seq_length_after_conv / adjusted_pool_size))\n",
    "\n",
    "        # Avisa se a dimensão ficou inválida e para de adicionar CNNs se for o caso\n",
    "        if current_seq_length <= 0 and i < n_cnn_layers - 1:\n",
    "            print(f\"Atenção: A camada Conv1D {i+1} resultou em uma dimensão de saída inválida ({current_seq_length}). As camadas CNN subsequentes foram ignoradas.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    n_lstm_layers = best_params['n_lstm_layers']\n",
    "    lstm_units = [best_params[f'lstm_units_{i}'] for i in range(n_lstm_layers)]\n",
    "    lstm_dropout = [best_params[f'lstm_dropout_{i}'] for i in range(n_lstm_layers)]\n",
    "    \n",
    "    n_dense_layers = best_params['n_dense_layers']\n",
    "    dense_units = [best_params[f'dense_units_{i}'] for i in range(n_dense_layers)]\n",
    "    dense_dropout = [best_params[f'dense_dropout_{i}'] for i in range(n_dense_layers)]\n",
    "    # Camadas LSTM\n",
    "    for i in range(n_lstm_layers):\n",
    "        return_sequences = (i < n_lstm_layers - 1)\n",
    "        model.add(layers.LSTM(\n",
    "            lstm_units[i],\n",
    "            return_sequences=return_sequences,\n",
    "            kernel_regularizer=l2(best_params['l2_weight'])\n",
    "        ))\n",
    "        model.add(layers.Dropout(lstm_dropout[i]))\n",
    "\n",
    "    # Camadas Densas\n",
    "    for i in range(n_dense_layers):\n",
    "        model.add(layers.Dense(dense_units[i], activation='relu'))\n",
    "        model.add(layers.Dropout(dense_dropout[i]))\n",
    "\n",
    "    # Saída\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.9897): #Limiar 0.9897\n",
    "    y_prob = model.predict(X_test).ravel()\n",
    "\n",
    "    # AUC e FPR\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Encontre o threshold ótimo (maximiza TPR - FPR)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    y_test_1d = y_test.squeeze()\n",
    "    y_pred = (y_prob > threshold).astype(int)\n",
    "    \n",
    "    # Métricas\n",
    "    print(classification_report(y_test_1d, y_pred))\n",
    "    print(pd.crosstab(y_test_1d, y_pred, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "    print(f'Threshold ótimo: {optimal_threshold:.4f}')\n",
    "    \n",
    "    return roc_auc, threshold, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processando Fundamental ===\n",
      "Estudo tcc_auc_all_features_CV_Fundamental_v2 encontrado. Verificando trials...\n",
      "Executando 195 trials restantes...\n",
      "=== Iniciando Fold 1 de 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748732321.824492 3182053 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:07:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748732330.006672 3184280 service.cc:152] XLA service 0x7a5224003c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748732330.006711 3184280 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "2025-05-31 19:58:50.312266: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748732331.735934 3184280 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/141\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49:02\u001b[0m 21s/step - accuracy: 0.5312 - auc: 0.3657 - loss: 34.5714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748732344.452363 3184280 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 142ms/step - accuracy: 0.5086 - auc: 0.5638 - loss: 39.0173 - val_accuracy: 0.6114 - val_auc: 0.6689 - val_loss: 31.5964 - learning_rate: 0.0080\n",
      "Epoch 2/100\n",
      "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5099 - auc: 0.5998 - loss: 28.5373 - val_accuracy: 0.1996 - val_auc: 0.4818 - val_loss: 21.4385 - learning_rate: 0.0080\n",
      "Epoch 3/100\n",
      "\u001b[1m 51/141\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4975 - auc: 0.6311 - loss: 20.5440"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "N_TRIALS = 200\n",
    "\n",
    "for dataset_name, data in datasets.items(): #datasets_seq.items(): se for LSTM ou CNN + LSTM\n",
    "    print(f'\\n=== Processando {dataset_name} ===')\n",
    "\n",
    "    study_name = f'tcc_auc_all_features_CV_{dataset_name.replace(\" \", \"_\")}_v2'\n",
    "    #study_name = f'tcc_auc_all_features_{dataset_name.replace(\" \", \"_\")}_lstm_v2'\n",
    "    #study_name = f'tcc_auc_all_features_{dataset_name.replace(\" \", \"_\")}_cnn_lstm_v1'\n",
    "    storage = 'sqlite:///../tcc.db'\n",
    "    model_path = f'models/{study_name}.keras'\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Modelo {model_path} já existe. Carregando modelo...\")\n",
    "        best_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        print(f'\\n=== Avaliação em {dataset_name} ===')\n",
    "        roc_auc, optimal_threshold, fpr, tpr = evaluate_model(best_model, data['X_test'], data['y_test'])\n",
    "        results[dataset_name] = {\n",
    "            'model': best_model,\n",
    "            'auc': roc_auc,\n",
    "            'threshold': optimal_threshold,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "        print(f\"Estudo {study_name} encontrado. Verificando trials...\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\"Estudo {study_name} não existe. Criando novo estudo...\")\n",
    "        study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            direction='maximize',\n",
    "            load_if_exists=True,\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            storage=storage\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    if (N_TRIALS - len(study.trials)) > 0:\n",
    "        print(f\"Executando {N_TRIALS - len(study.trials)} trials restantes...\")\n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, data['X_train'], data['y_train']),\n",
    "            n_trials=(N_TRIALS - len(study.trials)),\n",
    "            gc_after_trial=True,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Todos os trials já foram executados. Pulando otimização.\")\n",
    "    \n",
    "    # Passo 2: Treinar o modelo final com os melhores parâmetros\n",
    "    best_model = build_final_model_layer(study.best_params, data['X_train'].shape[1:]) #LSTM: build_final_model_lstm, #CNN + LSTM: build_final_model_cnn_lstm, #RNA: build_final_model_layer\n",
    "    final_model_callback = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=50,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    best_model.fit(\n",
    "        data['X_train'],\n",
    "        data['y_train'],\n",
    "        epochs=700,\n",
    "        batch_size=study.best_params['batch_size'],\n",
    "        validation_split=0.1,\n",
    "        callbacks=final_model_callback,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Salvar o modelo\n",
    "    print(f'\\n=== Salvando o modelo em models ===')\n",
    "    best_model.save(f'models/{study_name}.keras')\n",
    "    # Passo 3: Avaliar no teste\n",
    "    print(f'\\n=== Avaliação em {dataset_name} ===')\n",
    "    roc_auc, optimal_threshold, fpr, tpr = evaluate_model(best_model, data['X_test'], data['y_test'])\n",
    "    results[dataset_name] = {\n",
    "        'model': best_model,\n",
    "        'auc': roc_auc,\n",
    "        'threshold': optimal_threshold,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Treinando o modelo com os melhores parâmetros do Optuna -- OLD VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_study = 'sqlite:///tcc.db'\n",
    "study = optuna.load_study(\n",
    "    study_name='tcc_v5',\n",
    "    storage=storage_study\n",
    ")\n",
    "best_params = study.best_params\n",
    "print('Melhores hiperparâmetros:', best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_train, y_train, X_test, y_test, dataset):\n",
    "    study = optuna.load_study(\n",
    "        study_name=f'tcc_{dataset}_v5',\n",
    "        storage=storage_study\n",
    "    )\n",
    "    best_model = create_model(study.best_trial, X_train.shape[1:])\n",
    "    best_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=150,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    y_test_pred = (best_model.predict(X_test)).ravel()\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "    \n",
    "    print(f'AUC no teste: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=X_train_f0.shape[1:]))\n",
    "\n",
    "for _ in range(best_params['n_layers']):\n",
    "    model.add(layers.Dense(best_params['units'], activation='relu'))\n",
    "    model.add(layers.Dropout(best_params['dropout']))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    patience=75,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train_f0,\n",
    "    y_train_f0,\n",
    "    epochs=150,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    callbacks=[callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_f0 = model.predict(X_test_f0)\n",
    "y_pred_labels_f0 = (y_prob_f0 > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_f0, y_pred_labels_f0)\n",
    "print('Acurácia no conjunto de teste:', acc)\n",
    "print(classification_report(y_test_f0, y_pred_labels_f0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1d_f0 = y_test_f0.squeeze()\n",
    "y_pred_labels_1d_f0 = y_pred_labels_f0.squeeze()\n",
    "y_prob_1d_f0 = y_prob_f0.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(y_test_1d_f0, y_pred_labels_1d_f0, rownames=['Real'], colnames=['Predito'], margins=True),'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar = 0.9897\n",
    "y_pred_limiar_f0 = (y_prob_f0 > limiar)\n",
    "y_pred_limiar_f0 = np.multiply(y_pred_limiar_f0, 1)\n",
    "y_pred_limiar_f0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_limiar_1d_f0 = y_pred_limiar_f0.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn_f0, fp_f0, fn_f0, tp_f0 = confusion_matrix(y_test_1d_f0, y_pred_limiar_1d_f0).ravel()\n",
    "\n",
    "taxa_fp_f0 = fp_f0 / (fp_f0 + tn_f0)\n",
    "taxa_detec_f0 = tp_f0 / (tp_f0 + fn_f0)\n",
    "\n",
    "print('Taxa de Falsos Positivos:', taxa_fp_f0)\n",
    "print('Taxa de Detecção:', taxa_detec_f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_f0, tpr_f0, thresholds_f0 = roc_curve(y_test_1d_f0, y_prob_f0)\n",
    "roc_auc_f0 = auc(fpr_f0, tpr_f0)\n",
    "print('AUC:', roc_auc_f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr_f0, tpr_f0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0 e 2F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=X_train_h2.shape[1:]))\n",
    "\n",
    "for _ in range(best_params['n_layers']):\n",
    "    model.add(layers.Dense(best_params['units'], activation='relu'))\n",
    "    model.add(layers.Dropout(best_params['dropout']))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    patience=75,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train_h2,\n",
    "    y_train_h2,\n",
    "    epochs=150,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_h2 = model.predict(X_test_h2)\n",
    "y_pred_labels_h2 = (y_prob_h2 > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test_h2, y_pred_labels_h2)\n",
    "print('Acurácia no conjunto de teste:', acc)\n",
    "print(classification_report(y_test_h2, y_pred_labels_h2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1d_h2 = y_test_h2.squeeze()\n",
    "y_pred_labels_1d_h2 = y_pred_labels_h2.squeeze()\n",
    "y_prob_1d_h2 = y_prob_h2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(y_test_1d_h2, y_pred_labels_1d_h2, rownames=['Real'], colnames=['Predito'], margins=True),'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar = 0.9897\n",
    "y_pred_limiar_h2 = (y_prob_h2 > limiar)\n",
    "y_pred_limiar_h2 = np.multiply(y_pred_limiar_h2, 1)\n",
    "y_pred_limiar_h2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_limiar_1d_h2 = y_pred_limiar_h2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn_h2, fp_h2, fn_h2, tp_h2 = confusion_matrix(y_test_1d_h2, y_pred_limiar_1d_h2).ravel()\n",
    "\n",
    "taxa_fp_h2 = fp_h2 / (fp_h2 + tn_h2)\n",
    "taxa_detec_h2 = tp_h2 / (tp_h2 + fn_h2)\n",
    "\n",
    "print('Taxa de Falsos Positivos:', taxa_fp_h2)\n",
    "print('Taxa de Detecção:', taxa_detec_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_h2, tpr_h2, thresholds_h2 = roc_curve(y_test_1d_h2, y_prob_h2)\n",
    "roc_auc_h2 = auc(fpr_h2, tpr_h2)\n",
    "print('AUC:', roc_auc_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr_h2, tpr_h2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0, 2F0 e 3F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=X_train.shape[1:]))\n",
    "\n",
    "for _ in range(best_params['n_layers']):\n",
    "    model.add(layers.Dense(best_params['units'], activation='relu'))\n",
    "    model.add(layers.Dropout(best_params['dropout']))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',\n",
    "    patience=75,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=1,\n",
    "    callbacks=[callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(X_test)\n",
    "y_pred_labels = (y_prob > 0.5).astype(int)\n",
    "acc = accuracy_score(y_test, y_pred_labels)\n",
    "print('Acurácia no conjunto de teste:', acc)\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1d = y_test.squeeze()\n",
    "y_pred_labels_1d = y_pred_labels.squeeze()\n",
    "y_prob_1d = y_prob.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(y_test_1d, y_pred_labels_1d, rownames=['Real'], colnames=['Predito'], margins=True),'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar = 0.9897\n",
    "y_pred_limiar = (y_prob > limiar)\n",
    "y_pred_limiar = np.multiply(y_pred_limiar, 1)\n",
    "y_pred_limiar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_limiar_1d = y_pred_limiar.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_1d, y_pred_limiar_1d).ravel()\n",
    "\n",
    "taxa_fp = fp / (fp + tn)\n",
    "taxa_detec = tp / (tp + fn)\n",
    "\n",
    "print('Taxa de Falsos Positivos:', taxa_fp)\n",
    "print('Taxa de Detecção:', taxa_detec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_1d, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Métodos Tradicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rec = pd.read_csv('f0/X_test_rec_f0.csv')\n",
    "y_test_f0 = pd.read_parquet('f0/y_test_f0.parquet')\n",
    "\n",
    "X_test_rec_nh = X_test_rec.iloc[:, :500].values.astype(np.complex128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rec_nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "M = 500;\n",
    "alfa=0.05;\n",
    "\n",
    "teta = np.angle(X_test_rec_nh)\n",
    "y_test_only_1=np.flatnonzero(y_test)\n",
    "y_test_only_0=np.flatnonzero(y_test==0)\n",
    "## CSM\n",
    "CSM=(np.power((1/M)*np.sum(np.cos(teta),axis=1),2))+(np.power((1/M)*np.sum(np.sin(teta),axis=1),2))\n",
    "\n",
    "from scipy.stats.distributions import chi2\n",
    "\n",
    "VC_CSM=chi2.ppf(1-alfa, df=2)/(2*M)\n",
    "VC_CSM\n",
    "y_CSM_txd=(CSM[y_test_only_1] > VC_CSM)\n",
    "#y_pred_limiar=int(y_pred_limiar)\n",
    "\n",
    "# Converting boolean to integer\n",
    "y_CSM_txd = np.multiply(y_CSM_txd, 1)\n",
    "\n",
    "y_CSM_txd=np.mean(y_CSM_txd)\n",
    "\n",
    "\n",
    "y_CSM_fp=(CSM[y_test_only_0] > VC_CSM)\n",
    "#y_pred_limiar=int(y_pred_limiar)\n",
    "\n",
    "# Converting boolean to integer\n",
    "y_CSM_fp = np.multiply(y_CSM_fp, 1)\n",
    "\n",
    "y_CSM_fp=np.mean(y_CSM_fp)\n",
    "\n",
    "y_CSM=(CSM > VC_CSM)\n",
    "#y_pred_limiar=int(y_pred_limiar)\n",
    "\n",
    "# Converting boolean to integer\n",
    "y_CSM = np.multiply(y_CSM, 1)\n",
    "\n",
    "fpr_CSM, tpr_CSM, thresholds_CSM = roc_curve(y_test,CSM.transpose())\n",
    "print(y_CSM_txd)\n",
    "print(y_CSM_fp)\n",
    "## MSC\n",
    "MSC=(np.abs(np.sum(X_test_rec_nh,axis=1))**2)/(M*np.sum((np.abs(X_test_rec_nh)**2),axis=1))\n",
    "MSC = np.reshape(MSC,(1,len(MSC)))\n",
    "\n",
    "VC_MSC=1-(alfa**(1/(M-1)))\n",
    "VC_MSC\n",
    "y_MSC_txd=(MSC[0,y_test_only_1] > VC_MSC)\n",
    "#y_pred_limiar=int(y_pred_limiar)\n",
    "\n",
    "# Converting boolean to integer\n",
    "y_MSC_txd = np.multiply(y_MSC_txd, 1)\n",
    "\n",
    "y_MSC_txd=np.mean(y_MSC_txd)\n",
    "\n",
    "from numpy import matlib\n",
    "\n",
    "y_MSC_fp=(MSC[0, y_test_only_0] > np.matlib.repmat(VC_MSC,1096,1))\n",
    "#y_pred_limiar=int(y_pred_limiar)\n",
    "\n",
    "# Converting boolean to integer\n",
    "y_MSC_fp = np.multiply(y_MSC_fp, 1)\n",
    "\n",
    "y_MSC_fp=np.mean(y_MSC_fp)\n",
    "\n",
    "fpr_MSC, tpr_MSC, thresholds_MSC = roc_curve(y_test,MSC.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que você já tenha as curvas externas:\n",
    "results['CSM'] = {\n",
    "    'fpr': fpr_CSM, \n",
    "    'tpr': tpr_CSM, \n",
    "    'auc': auc(fpr_CSM, tpr_CSM),\n",
    "    'label': 'CSM'\n",
    "}\n",
    "\n",
    "results['MSC'] = {\n",
    "    'fpr': fpr_MSC, \n",
    "    'tpr': tpr_MSC, \n",
    "    'auc': auc(fpr_MSC, tpr_MSC),\n",
    "    'label': 'MSC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva ROC - Comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_roc_curves(results):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plotar cada curva do dicionário\n",
    "    for key in results:\n",
    "        data = results[key]\n",
    "        plt.plot(\n",
    "            data['fpr'], \n",
    "            data['tpr'], \n",
    "            linewidth=2,\n",
    "            label=f\"{data.get('label', key)} (AUC = {data['auc']:.2f})\"\n",
    "        )\n",
    "    \n",
    "    # Linha de referência\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Comparação de Curvas ROC', fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Após o loop de avaliação dos datasets, adicione:\n",
    "plot_all_roc_curves(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
